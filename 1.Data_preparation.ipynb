{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qq twint\n",
    "# !pip install -qq sastrawi\n",
    "# !pip install -qq nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Function <em>scrapKeyword(keyword, since=None, save=True)</em></li>\n",
    "<li>Function <em>scrapUser(username, keyword=None, since=None, save=True)</em></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 70 Tweets from @detikcom.\n"
     ]
    }
   ],
   "source": [
    "from utils.twint_scrapper import scrapKeyword, scrapUser\n",
    "\n",
    "# Setup keyword\n",
    "keyword = 'minyak goreng'\n",
    "\n",
    "## Choose one of the methods below (comment another)!\n",
    "# Scrap by keyword\n",
    "# df1 = scrapKeyword(keyword=keyword, save=True) #comment to disable\n",
    "\n",
    "# Scrap by user (*without @) and keyword (*if needed)\n",
    "df2 = scrapUser(username='detikcom', keyword=keyword, save=True) #comment to disable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Data <em>(if needed)</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(651, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.twint_scrapper import saveData, mergeData\n",
    "\n",
    "# List data to merge\n",
    "data_to_merge = [\n",
    "                    'data/raw/2022-04-12-minyak goreng-detikcom.csv', \n",
    "                    'data/raw/2022-04-12-minyak goreng-kumparan.csv',\n",
    "                    'data/raw/2022-04-12-minyak goreng-cnnindonesia.csv',\n",
    "                    'data/raw/2022-04-12-minyak goreng-tribunnews.csv',\n",
    "                    'data/raw/2022-04-12-minyak goreng-liputan6dotcom.csv',\n",
    "                    'data/raw/2022-04-12-minyak goreng-kompascom.csv',\n",
    "                    'data/raw/2022-04-12-minyak goreng-merdekadotcom.csv'\n",
    "                ]\n",
    "\n",
    "# Merge data\n",
    "df_merge = mergeData(data_to_merge)\n",
    "# Save merged data\n",
    "saveData(df_merge, \"merged_data_minyak_goreng\")\n",
    "df_merge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Full Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (before): 651\n",
      "Duplicate: 0\n",
      "Total (after): 651\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 651 entries, 0 to 650\n",
      "Data columns (total 38 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               651 non-null    int64  \n",
      " 1   conversation_id  651 non-null    int64  \n",
      " 2   created_at       651 non-null    float64\n",
      " 3   date             651 non-null    object \n",
      " 4   timezone         651 non-null    int64  \n",
      " 5   place            0 non-null      float64\n",
      " 6   tweet            651 non-null    object \n",
      " 7   language         651 non-null    object \n",
      " 8   hashtags         651 non-null    object \n",
      " 9   cashtags         651 non-null    object \n",
      " 10  user_id          651 non-null    int64  \n",
      " 11  user_id_str      651 non-null    int64  \n",
      " 12  username         651 non-null    object \n",
      " 13  name             651 non-null    object \n",
      " 14  day              651 non-null    int64  \n",
      " 15  hour             651 non-null    int64  \n",
      " 16  link             651 non-null    object \n",
      " 17  urls             651 non-null    object \n",
      " 18  photos           651 non-null    object \n",
      " 19  video            651 non-null    int64  \n",
      " 20  thumbnail        32 non-null     object \n",
      " 21  retweet          651 non-null    bool   \n",
      " 22  nlikes           651 non-null    int64  \n",
      " 23  nreplies         651 non-null    int64  \n",
      " 24  nretweets        651 non-null    int64  \n",
      " 25  quote_url        0 non-null      float64\n",
      " 26  search           651 non-null    object \n",
      " 27  near             0 non-null      float64\n",
      " 28  geo              0 non-null      float64\n",
      " 29  source           0 non-null      float64\n",
      " 30  user_rt_id       0 non-null      float64\n",
      " 31  user_rt          0 non-null      float64\n",
      " 32  retweet_id       0 non-null      float64\n",
      " 33  reply_to         651 non-null    object \n",
      " 34  retweet_date     0 non-null      float64\n",
      " 35  translate        0 non-null      float64\n",
      " 36  trans_src        0 non-null      float64\n",
      " 37  trans_dest       0 non-null      float64\n",
      "dtypes: bool(1), float64(13), int64(11), object(13)\n",
      "memory usage: 193.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Setup Main Dataframe\n",
    "df = df_merge\n",
    "\n",
    "# Remove Duplicate\n",
    "print('Total (before):',len(df))\n",
    "print('Duplicate:',df.duplicated(subset='id').sum())\n",
    "df = df.drop_duplicates(['id'])\n",
    "print('Total (after):',len(df))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformate Date\n",
    "df['date'] = df.date.astype('datetime64[ns]')\n",
    "\n",
    "# Remove Unused Columns\n",
    "df = df[['id','conversation_id','date','timezone','place','tweet','language','hashtags', 'user_id', 'username','name','day','hour','link','retweet','nlikes','nreplies','nretweets','search','user_rt_id','user_rt','retweet_id','retweet_date','reply_to']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text/Tweet Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\calvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 651/651 [00:00<00:00, 295411.87it/s]\n",
      "100%|██████████| 651/651 [00:00<00:00, 3047.83it/s]\n",
      "100%|██████████| 651/651 [01:29<00:00,  7.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import getMentioned, cleanText, stemmingClear\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['mentioned']   = df.tweet.progress_apply(getMentioned)\n",
    "df['tweet_clean'] = df.tweet.progress_apply(cleanText)\n",
    "df['tweet_stem']  = df.tweet_clean.progress_apply(stemmingClear)\n",
    "df_tweet = df[['id','date','username','tweet','tweet_clean','tweet_stem']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir   = r'data/clean/'\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "# Save Main Dataset\n",
    "df.to_csv(f'{dir}{keyword.replace(\" \",\"_\")}.csv', encoding='utf-8')\n",
    "\n",
    "# Save Text Dataset\n",
    "df_tweet.to_csv(f'{dir}tweet-{keyword.replace(\" \",\"_\")}.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c59d4e3b730d95359861c54fe846ec5e9693e27df65d87b24e87be7d1076b66e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
